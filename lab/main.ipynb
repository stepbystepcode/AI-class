{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通用模型摄像头识别\n",
    "![通用模型摄像头识别](./resources/general-webcam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 3 chairs, 2 laptops, 43.3ms\n",
      "Speed: 3.2ms preprocess, 43.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 2 laptops, 380.1ms\n",
      "Speed: 3.4ms preprocess, 380.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 3 laptops, 101.8ms\n",
      "Speed: 3.0ms preprocess, 101.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 laptop, 116.1ms\n",
      "Speed: 4.5ms preprocess, 116.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 2 laptops, 72.9ms\n",
      "Speed: 3.3ms preprocess, 72.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 2 laptops, 59.8ms\n",
      "Speed: 2.3ms preprocess, 59.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 laptop, 58.2ms\n",
      "Speed: 3.0ms preprocess, 58.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 laptop, 65.5ms\n",
      "Speed: 2.4ms preprocess, 65.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 laptop, 86.0ms\n",
      "Speed: 3.6ms preprocess, 86.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 laptop, 1 book, 58.7ms\n",
      "Speed: 2.9ms preprocess, 58.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 chairs, 1 laptop, 39.6ms\n",
      "Speed: 3.0ms preprocess, 39.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 laptop, 38.1ms\n",
      "Speed: 2.0ms preprocess, 38.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 laptop, 1 book, 39.9ms\n",
      "Speed: 1.7ms preprocess, 39.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 laptop, 41.5ms\n",
      "Speed: 2.2ms preprocess, 41.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 laptop, 39.5ms\n",
      "Speed: 1.8ms preprocess, 39.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 chair, 1 laptop, 38.9ms\n",
      "Speed: 1.9ms preprocess, 38.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"./model/yolo11n.pt\")\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        results = model(frame)\n",
    "        annotated_frame = results[0].plot()\n",
    "        cv2.imshow(\"YOLO Inference\", annotated_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 通用模型识别本地图片\n",
    "![通用模型识别本地图片](./resources/general-pic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/stepbystep/Documents/AI-class/lab/test/test.png: 352x640 2 persons, 1 backpack, 1 laptop, 2 books, 44.9ms\n",
      "Speed: 3.5ms preprocess, 44.9ms inference, 1.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([ 0.,  0., 24., 63., 73., 73.])\n",
      "conf: tensor([0.8742, 0.4147, 0.3586, 0.3586, 0.2847, 0.2684])\n",
      "data: tensor([[2.5845e+02, 1.5790e+02, 1.2137e+03, 8.6446e+02, 8.7419e-01, 0.0000e+00],\n",
      "        [5.9669e-01, 4.3276e+02, 3.4523e+02, 8.5983e+02, 4.1468e-01, 0.0000e+00],\n",
      "        [8.6530e+01, 6.8506e+02, 3.4720e+02, 8.6642e+02, 3.5863e-01, 2.4000e+01],\n",
      "        [1.4957e+03, 6.7928e+02, 1.6084e+03, 8.0835e+02, 3.5860e-01, 6.3000e+01],\n",
      "        [1.1497e+03, 5.0836e+02, 1.2003e+03, 6.1739e+02, 2.8467e-01, 7.3000e+01],\n",
      "        [1.1795e+03, 5.0837e+02, 1.2057e+03, 6.0970e+02, 2.6838e-01, 7.3000e+01]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (868, 1686)\n",
      "shape: torch.Size([6, 6])\n",
      "xywh: tensor([[ 736.0776,  511.1809,  955.2480,  706.5617],\n",
      "        [ 172.9122,  646.2958,  344.6310,  427.0724],\n",
      "        [ 216.8637,  775.7380,  260.6680,  181.3561],\n",
      "        [1552.0468,  743.8165,  112.7397,  129.0751],\n",
      "        [1174.9962,  562.8772,   50.5664,  109.0352],\n",
      "        [1192.6121,  559.0379,   26.1506,  101.3315]])\n",
      "xywhn: tensor([[0.4366, 0.5889, 0.5666, 0.8140],\n",
      "        [0.1026, 0.7446, 0.2044, 0.4920],\n",
      "        [0.1286, 0.8937, 0.1546, 0.2089],\n",
      "        [0.9205, 0.8569, 0.0669, 0.1487],\n",
      "        [0.6969, 0.6485, 0.0300, 0.1256],\n",
      "        [0.7074, 0.6441, 0.0155, 0.1167]])\n",
      "xyxy: tensor([[2.5845e+02, 1.5790e+02, 1.2137e+03, 8.6446e+02],\n",
      "        [5.9669e-01, 4.3276e+02, 3.4523e+02, 8.5983e+02],\n",
      "        [8.6530e+01, 6.8506e+02, 3.4720e+02, 8.6642e+02],\n",
      "        [1.4957e+03, 6.7928e+02, 1.6084e+03, 8.0835e+02],\n",
      "        [1.1497e+03, 5.0836e+02, 1.2003e+03, 6.1739e+02],\n",
      "        [1.1795e+03, 5.0837e+02, 1.2057e+03, 6.0970e+02]])\n",
      "xyxyn: tensor([[1.5329e-01, 1.8191e-01, 7.1987e-01, 9.9592e-01],\n",
      "        [3.5391e-04, 4.9857e-01, 2.0476e-01, 9.9059e-01],\n",
      "        [5.1323e-02, 7.8924e-01, 2.0593e-01, 9.9818e-01],\n",
      "        [8.8712e-01, 7.8258e-01, 9.5398e-01, 9.3128e-01],\n",
      "        [6.8192e-01, 5.8567e-01, 7.1191e-01, 7.1128e-01],\n",
      "        [6.9961e-01, 5.8568e-01, 7.1512e-01, 7.0242e-01]])\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"./model/yolo11n.pt\")\n",
    "results = model('./test/test.png')\n",
    "for r in results:\n",
    "    print(r.boxes)\n",
    "    r.show()\n",
    "    r.save('./test/general-pic.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自训练微调锥形路障识别模型\n",
    "![自训练微调锥形路障识别模型](./resources/finetune-video.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 6 Safety Cones, 357.6ms\n",
      "Speed: 13.4ms preprocess, 357.6ms inference, 159.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 20.4ms\n",
      "Speed: 4.3ms preprocess, 20.4ms inference, 43.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-18 19:11:20.661 python[64612:11508877] _TIPropertyValueIsValid called with 16 on nil context!\n",
      "2024-11-18 19:11:20.661 python[64612:11508877] imkxpc_getApplicationProperty:reply: called with incorrect property value 16, bailing.\n",
      "2024-11-18 19:11:20.661 python[64612:11508877] Text input context does not respond to _valueForTIProperty:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 5 Safety Cones, 15.2ms\n",
      "Speed: 3.0ms preprocess, 15.2ms inference, 79.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 19.5ms\n",
      "Speed: 2.6ms preprocess, 19.5ms inference, 58.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 2\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 21.2ms\n",
      "Speed: 2.9ms preprocess, 21.2ms inference, 10.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 16.5ms\n",
      "Speed: 2.4ms preprocess, 16.5ms inference, 64.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 23.5ms\n",
      "Speed: 2.8ms preprocess, 23.5ms inference, 12.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 21.5ms\n",
      "Speed: 2.6ms preprocess, 21.5ms inference, 7.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 16.2ms\n",
      "Speed: 2.7ms preprocess, 16.2ms inference, 39.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 14.3ms\n",
      "Speed: 2.7ms preprocess, 14.3ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 14.3ms\n",
      "Speed: 2.6ms preprocess, 14.3ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 17.4ms\n",
      "Speed: 2.7ms preprocess, 17.4ms inference, 36.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 2\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 18.3ms\n",
      "Speed: 2.7ms preprocess, 18.3ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 2\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 13.5ms\n",
      "Speed: 2.5ms preprocess, 13.5ms inference, 42.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 2\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 13.0ms\n",
      "Speed: 2.6ms preprocess, 13.0ms inference, 38.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 14.1ms\n",
      "Speed: 2.4ms preprocess, 14.1ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 7 Safety Cones, 17.4ms\n",
      "Speed: 2.6ms preprocess, 17.4ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 3\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 7\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 16.5ms\n",
      "Speed: 2.5ms preprocess, 16.5ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 16.3ms\n",
      "Speed: 4.2ms preprocess, 16.3ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 16.9ms\n",
      "Speed: 2.2ms preprocess, 16.9ms inference, 21.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 1\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 4 Safety Cones, 16.8ms\n",
      "Speed: 2.5ms preprocess, 16.8ms inference, 40.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 2\n",
      "All: 4\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 4 Safety Cones, 16.2ms\n",
      "Speed: 4.2ms preprocess, 16.2ms inference, 38.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 2\n",
      "All: 4\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 16.7ms\n",
      "Speed: 2.6ms preprocess, 16.7ms inference, 21.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 17.1ms\n",
      "Speed: 2.3ms preprocess, 17.1ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 16.2ms\n",
      "Speed: 2.8ms preprocess, 16.2ms inference, 6.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 17.3ms\n",
      "Speed: 2.6ms preprocess, 17.3ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 7 Safety Cones, 12.7ms\n",
      "Speed: 3.5ms preprocess, 12.7ms inference, 21.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 3\n",
      "Yellow Count: 4\n",
      "All: 7\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 7 Safety Cones, 13.5ms\n",
      "Speed: 2.3ms preprocess, 13.5ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 3\n",
      "Yellow Count: 4\n",
      "All: 7\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 9 Safety Cones, 13.4ms\n",
      "Speed: 2.2ms preprocess, 13.4ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 4\n",
      "Yellow Count: 4\n",
      "All: 9\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 8 Safety Cones, 13.5ms\n",
      "Speed: 2.5ms preprocess, 13.5ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 3\n",
      "Yellow Count: 4\n",
      "All: 8\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 7 Safety Cones, 16.8ms\n",
      "Speed: 2.4ms preprocess, 16.8ms inference, 37.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 3\n",
      "Yellow Count: 4\n",
      "All: 7\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 7 Safety Cones, 13.4ms\n",
      "Speed: 2.5ms preprocess, 13.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 3\n",
      "Yellow Count: 4\n",
      "All: 7\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 13.4ms\n",
      "Speed: 2.7ms preprocess, 13.4ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 3\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 14.0ms\n",
      "Speed: 2.7ms preprocess, 14.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 3\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 16.7ms\n",
      "Speed: 2.5ms preprocess, 16.7ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 3\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 16.4ms\n",
      "Speed: 2.4ms preprocess, 16.4ms inference, 39.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 3\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 17.1ms\n",
      "Speed: 2.5ms preprocess, 17.1ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 16.1ms\n",
      "Speed: 2.5ms preprocess, 16.1ms inference, 22.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 17.1ms\n",
      "Speed: 2.6ms preprocess, 17.1ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 13.5ms\n",
      "Speed: 3.1ms preprocess, 13.5ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 12.9ms\n",
      "Speed: 2.6ms preprocess, 12.9ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 13.7ms\n",
      "Speed: 2.9ms preprocess, 13.7ms inference, 38.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 16.0ms\n",
      "Speed: 2.4ms preprocess, 16.0ms inference, 51.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 16.6ms\n",
      "Speed: 2.5ms preprocess, 16.6ms inference, 22.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 3\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 16.8ms\n",
      "Speed: 2.5ms preprocess, 16.8ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 4\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 13.5ms\n",
      "Speed: 3.2ms preprocess, 13.5ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 4\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 14.1ms\n",
      "Speed: 2.4ms preprocess, 14.1ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 4\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 13.7ms\n",
      "Speed: 2.6ms preprocess, 13.7ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 4\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 16.8ms\n",
      "Speed: 2.3ms preprocess, 16.8ms inference, 6.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 4\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 4 Safety Cones, 16.1ms\n",
      "Speed: 3.0ms preprocess, 16.1ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 2\n",
      "All: 4\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 17.6ms\n",
      "Speed: 2.5ms preprocess, 17.6ms inference, 7.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 2\n",
      "Yellow Count: 2\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 15.9ms\n",
      "Speed: 2.7ms preprocess, 15.9ms inference, 7.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 2\n",
      "Yellow Count: 2\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 17.7ms\n",
      "Speed: 2.4ms preprocess, 17.7ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 2\n",
      "Yellow Count: 2\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 13.7ms\n",
      "Speed: 2.4ms preprocess, 13.7ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 2\n",
      "Yellow Count: 2\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 13.5ms\n",
      "Speed: 2.8ms preprocess, 13.5ms inference, 38.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 2\n",
      "Blue Count: 2\n",
      "Yellow Count: 2\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 4 Safety Cones, 13.2ms\n",
      "Speed: 2.4ms preprocess, 13.2ms inference, 5.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 2\n",
      "Yellow Count: 2\n",
      "All: 4\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 14.1ms\n",
      "Speed: 2.4ms preprocess, 14.1ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 3\n",
      "Yellow Count: 2\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 16.7ms\n",
      "Speed: 2.8ms preprocess, 16.7ms inference, 6.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 3\n",
      "Yellow Count: 2\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 6 Safety Cones, 17.2ms\n",
      "Speed: 2.3ms preprocess, 17.2ms inference, 12.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 3\n",
      "Yellow Count: 2\n",
      "All: 6\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 16.8ms\n",
      "Speed: 2.6ms preprocess, 16.8ms inference, 6.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 0\n",
      "Blue Count: 3\n",
      "Yellow Count: 2\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 17.3ms\n",
      "Speed: 2.2ms preprocess, 17.3ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 2\n",
      "Yellow Count: 2\n",
      "All: 5\n",
      "------------------------------\n",
      "\n",
      "0: 384x640 5 Safety Cones, 16.0ms\n",
      "Speed: 2.4ms preprocess, 16.0ms inference, 7.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Red Count: 1\n",
      "Blue Count: 2\n",
      "Yellow Count: 2\n",
      "All: 5\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# https://docs.ultralytics.com/modes/track\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('./model/best.pt')\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# https://www.youtube.com/watch?v=oWMXQkGOzho\n",
    "cap = cv2.VideoCapture('./test/test.mp4')\n",
    "# cap = cv2.VideoCapture(0)\n",
    "lower_red = np.array([0, 100, 100])\n",
    "upper_red = np.array([10, 255, 255])\n",
    "lower_blue = np.array([100, 100, 100])\n",
    "upper_blue = np.array([130, 255, 255])\n",
    "lower_yellow = np.array([20, 100, 100])\n",
    "upper_yellow = np.array([30, 255, 255])\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        red_count = 0\n",
    "        blue_count = 0\n",
    "        yellow_count = 0\n",
    "        results = model.track(frame, persist=True, tracker=\"bytetrack.yaml\")\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "                id = int(box.id[0]) if box.id is not None else None\n",
    "                roi = frame[y1:y2, x1:x2]\n",
    "                hsv_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "                red_mask = cv2.inRange(hsv_roi, lower_red, upper_red)\n",
    "                blue_mask = cv2.inRange(hsv_roi, lower_blue, upper_blue)\n",
    "                yellow_mask = cv2.inRange(hsv_roi, lower_yellow, upper_yellow)\n",
    "                red_pixels = cv2.countNonZero(red_mask)\n",
    "                blue_pixels = cv2.countNonZero(blue_mask)\n",
    "                yellow_pixels = cv2.countNonZero(yellow_mask)\n",
    "                max_pixels = max(red_pixels, blue_pixels, yellow_pixels)\n",
    "                if max_pixels == red_pixels:\n",
    "                    color = \"Red\"\n",
    "                    color_box = (0, 0, 255)\n",
    "                    background_color = (0, 0, 255)\n",
    "                    red_count += 1\n",
    "                elif max_pixels == blue_pixels:\n",
    "                    color = \"Blue\"\n",
    "                    color_box = (255, 0, 0)\n",
    "                    background_color = (255, 0, 0)\n",
    "                    blue_count += 1\n",
    "                else:\n",
    "                    color = \"Yellow\"\n",
    "                    color_box = (0, 255, 255)\n",
    "                    background_color = (0, 255, 255)\n",
    "                    yellow_count += 1\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color_box, 4)\n",
    "                label = f\"id: {id} {color}\" if id is not None else color\n",
    "                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.9, 2)[0]\n",
    "                cv2.rectangle(frame, (x1, y1 - label_size[1] - 10), (x1 + label_size[0], y1), background_color, -1)\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 3)\n",
    "        \n",
    "        total_count = red_count + blue_count + yellow_count\n",
    "        print(f\"Red Count: {red_count}\")\n",
    "        print(f\"Blue Count: {blue_count}\")\n",
    "        print(f\"Yellow Count: {yellow_count}\")\n",
    "        print(f\"All: {total_count}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        cv2.imshow(\"YOLO11 Tracking\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练过程\n",
    "> 日志: ./train.log\n",
    "## 训练效果\n",
    "![模型训练效果](./runs/detect/train8/results.png)\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "  <img src=\"./runs/detect/train8/confusion_matrix.png\" alt=\"Image 1\" style=\"width: 50%;\"/>\n",
    "  <img src=\"./runs/detect/train8/confusion_matrix_normalized.png\" alt=\"Image 2\" style=\"width: 50%;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "<img src=\"./runs/detect/train8/F1_curve.png\" alt=\"Image 1\" style=\"width: 50%;\"/>\n",
    "<img src=\"./runs/detect/train8/P_curve.png\" alt=\"Image 2\" style=\"width: 50%;\"/>\n",
    "</div>\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "<img src=\"./runs/detect/train8/PR_curve.png\" alt=\"Image 3\" style=\"width: 50%;\"/>\n",
    "<img src=\"./runs/detect/train8/R_curve.png\" alt=\"Image 4\" style=\"width: 50%;\"/>\n",
    "</div>\n",
    "\n",
    "![](./runs/detect/train8/labels.jpg)\n",
    "![](./runs/detect/train8/labels_correlogram.jpg)\n",
    "![](./runs/detect/train8/train_batch0.jpg)\n",
    "![](./runs/detect/train8/train_batch1.jpg)\n",
    "![](./runs/detect/train8/train_batch16830.jpg)\n",
    "![](./runs/detect/train8/train_batch16831.jpg)\n",
    "![](./runs/detect/train8/train_batch16832.jpg)\n",
    "![](./runs/detect/train8/train_batch2.jpg)\n",
    "![](./runs/detect/train8/val_batch0_labels.jpg)\n",
    "![](./runs/detect/train8/val_batch0_pred.jpg)\n",
    "![](./runs/detect/train8/val_batch1_labels.jpg)\n",
    "![](./runs/detect/train8/val_batch1_pred.jpg)\n",
    "![](./runs/detect/train8/val_batch2_labels.jpg)\n",
    "![](./runs/detect/train8/val_batch2_pred.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反思\n",
    "\n",
    "---\n",
    "\n",
    "### **优点**\n",
    "1. **逐渐降低的损失**：\n",
    "   - `train/box_loss`、`train/cls_loss` 和 `train/dfl_loss` 在绝大多数 epoch 中都在逐渐减小，表明模型的优化方向正确，损失函数有效。\n",
    "   - 验证损失 (`val/box_loss`、`val/cls_loss` 和 `val/dfl_loss`) 同样在减小，说明模型具有一定的泛化能力。\n",
    "\n",
    "2. **较好的精度与召回率**：\n",
    "   - `metrics/precision(B)` 达到 **0.78**，说明预测的目标中大部分是准确的。\n",
    "   - `metrics/recall(B)` 达到 **0.65**，说明模型捕获了大部分的真实目标。\n",
    "   - 这对许多实际场景（如目标检测应用）来说是可以接受的。\n",
    "\n",
    "3. **mAP50 和 mAP50-95**：\n",
    "   - `metrics/mAP50(B)` 达到 **0.71+**，表明模型在 IoU=50% 阈值下具有较高的目标检测性能。\n",
    "   - `metrics/mAP50-95(B)` 达到 **0.37+**，反映模型在不同 IoU 阈值下的平均性能，虽然不算特别高，但可以接受。\n",
    "\n",
    "---\n",
    "\n",
    "### **不足**\n",
    "1. **验证损失较高**：\n",
    "   - 虽然验证损失（`val/box_loss`、`val/cls_loss` 等）在逐渐减小，但比训练损失高，表明模型存在 **过拟合倾向**。\n",
    "   - 尤其在后期，`val/box_loss` 稍微有增加趋势，暗示可能需要更强的正则化手段（如增加 `weight_decay` 或减小 `batch_size`）。\n",
    "\n",
    "2. **召回率略低**：\n",
    "   - `metrics/recall(B)` 只有 **0.65**，说明有部分目标未被检测到。\n",
    "   - 如果在实际应用中对漏检容忍度低（如安全性检测场景），需要进一步优化召回率。\n",
    "\n",
    "3. **mAP50-95 较低**：\n",
    "   - `metrics/mAP50-95(B)` 在 **0.37** 左右，表明模型对小目标或者边界不够准确。\n",
    "   - 这可能与数据分布（目标大小、复杂度）或损失函数权重设置相关。\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 第一行（训练过程指标）\n",
    "\n",
    "1. **`train/box_loss`**  \n",
    "   - **含义**：表示边界框回归损失（Box Loss）。反映模型预测的边界框与真实目标框之间的差距。  \n",
    "   - **目标**：越低越好，表明模型对目标位置的预测更加准确。\n",
    "\n",
    "2. **`train/cls_loss`**  \n",
    "   - **含义**：表示分类损失（Classification Loss）。反映模型对每个目标的类别预测是否准确。  \n",
    "   - **目标**：越低越好，表明模型能够更好地区分不同类别。\n",
    "\n",
    "3. **`train/dfl_loss`**  \n",
    "   - **含义**：表示分布聚合损失（Distribution Focal Loss），通常用于预测边界框的更精确的定位分布。  \n",
    "   - **目标**：越低越好，说明模型在预测目标框边界的质量更高。\n",
    "\n",
    "4. **`metrics/precision(B)`**  \n",
    "   - **含义**：精度（Precision），即模型预测为正样本的目标中，真正是正样本的比例。  \n",
    "   - **目标**：越高越好，表明模型预测结果更精确，误报少。\n",
    "\n",
    "5. **`metrics/recall(B)`**  \n",
    "   - **含义**：召回率（Recall），即真实正样本中被模型正确预测为正样本的比例。  \n",
    "   - **目标**：越高越好，表明模型捕捉到更多的真实目标。\n",
    "\n",
    "### 第二行（验证过程指标）\n",
    "\n",
    "6. **`val/box_loss`**  \n",
    "   - **含义**：验证集上的边界框损失，与训练集的 `train/box_loss` 类似，但作用于验证数据。  \n",
    "   - **目标**：越低越好，表示模型在验证集上对目标位置预测更加准确。\n",
    "\n",
    "7. **`val/cls_loss`**  \n",
    "   - **含义**：验证集上的分类损失，与训练集的 `train/cls_loss` 类似，但作用于验证数据。  \n",
    "   - **目标**：越低越好，表示模型在验证集上的分类能力更强。\n",
    "\n",
    "8. **`val/dfl_loss`**  \n",
    "   - **含义**：验证集上的分布聚合损失，与训练集的 `train/dfl_loss` 类似，但作用于验证数据。  \n",
    "   - **目标**：越低越好，表示模型对验证集目标框边界的预测质量更高。\n",
    "\n",
    "9. **`metrics/mAP50(B)`**  \n",
    "   - **含义**：在验证集上计算的平均精度（mAP），用 IoU（交并比）阈值 50% 来衡量目标检测的整体性能。  \n",
    "   - **目标**：越高越好，表明模型整体检测精度更高。\n",
    "\n",
    "10. **`metrics/mAP50-95(B)`**  \n",
    "    - **含义**：在验证集上计算的平均精度（mAP），IoU 阈值从 50% 到 95% 的范围内取平均值，综合反映模型在不同 IoU 水平下的性能。  \n",
    "    - **目标**：越高越好，表示模型对目标的检测能力更全面。\n",
    "\n",
    "### 总体理解\n",
    "- **损失项**（`box_loss`、`cls_loss`、`dfl_loss`）：越低越好，表示模型预测误差更小。\n",
    "- **性能指标**（`precision`、`recall`、`mAP50`、`mAP50-95`）：越高越好，表示模型性能更好。\n",
    "- **验证指标 vs. 训练指标**：验证指标反映模型对未见数据的泛化能力，应与训练指标保持一致或接近。如果差距过大，可能存在过拟合问题。\n",
    "\n",
    "---\n",
    "\n",
    "### **总体**\n",
    "模型 **性能中等偏好**，特别在 `mAP50` 上已经达到较高水平（0.71+）。不过，对于一些严格场景（如需要高召回率和小目标检测的任务），还需进一步优化。\n",
    "\n",
    "通过修改超参数、调整数据增强策略、增加数据量等方式，可以进一步提升模型性能。经过几轮训练，模型的性能有所提升。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
